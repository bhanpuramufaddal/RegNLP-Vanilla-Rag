import os
import faiss
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from sentence_transformers import SentenceTransformer

# Step 1: Load the documents
def load_documents_from_folder(folder_path):
    documents = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".txt"):  # Only process .txt files
            file_path = os.path.join(folder_path, filename)
            with open(file_path, "r") as file:
                documents.extend(file.readlines())
    return documents

# Step 2: Generate Embeddings for Documents
def embed_documents(documents, embedding_model):
    embeddings = embedding_model.encode(documents, convert_to_tensor=True)
    return embeddings.cpu().numpy()  # FAISS requires numpy arrays

# Step 3: Build FAISS Vector Store
def build_faiss_index(embeddings):
    index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance metric
    index.add(embeddings)
    return index

# Step 4: Query the Vector Store
def retrieve_documents(query, embedding_model, index, documents, top_k=3):
    query_embedding = embedding_model.encode([query], convert_to_tensor=True).cpu().numpy()
    distances, indices = index.search(query_embedding, top_k)
    return [documents[i] for i in indices[0]]

# Step 5: Generate Answer Using a Language Model
def generate_answer(question, context, model_name):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    nlp_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer)
    input_text = f"Answer this question based on the context:\n\nContext: {context}\n\nQuestion: {question}\nAnswer:"
    response = nlp_pipeline(input_text, max_length=200, num_return_sequences=1)
    return response[0]["generated_text"]

# Main Function
def main():
    # Load and prepare documents
    file_path = "/Users/sakshivatsa/Downloads/RegNLPDocuments"
    documents = load_documents_from_folder(file_path)

    print('Load documents step complete')
    
    # Use Sentence Transformers for embeddings
    embedding_model = SentenceTransformer("models/paraphrase-MiniLM-L6-v2")
    print('Model loaded')
    
    document_embeddings = embed_documents(documents, embedding_model)
    print('Document embeddings created')

    # Build FAISS index
    faiss_index = build_faiss_index(document_embeddings)
    print('Faiss Index - ', faiss_index)

    # Interact with the system
    print("Vanilla RAG System is ready. Type your question or 'exit' to quit.")
    while True:
        query = input("Question: ")
        if query.lower() == "exit":
            break

        # Retrieve relevant documents
        top_documents = retrieve_documents(query, embedding_model, faiss_index, documents)
        context = " ".join(top_documents)

        # Generate an answer
        answer = generate_answer(query, context)
        print(f"Answer: {answer}\n")

if __name__ == "__main__":
    main()







